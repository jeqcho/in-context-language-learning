{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First run the conversion script at convert.sh with the folder of your checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home07/jchooi/.conda/envs/olmo2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model_path = \"/n/holyscratch01/sham_lab/summer_2024/checkpoints/test-batch-1024-39337249/step3000-unsharded\"\n",
    "model_path += \"-hf\"\n",
    "olmo = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OlmoForCausalLM(\n",
       "  (model): OlmoModel(\n",
       "    (embed_tokens): Embedding(128, 16, padding_idx=7)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x OlmoDecoderLayer(\n",
       "        (self_attn): OlmoSdpaAttention(\n",
       "          (q_proj): Linear(in_features=16, out_features=16, bias=False)\n",
       "          (k_proj): Linear(in_features=16, out_features=16, bias=False)\n",
       "          (v_proj): Linear(in_features=16, out_features=16, bias=False)\n",
       "          (o_proj): Linear(in_features=16, out_features=16, bias=False)\n",
       "          (rotary_emb): OlmoRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): OlmoMLP(\n",
       "          (gate_proj): Linear(in_features=16, out_features=8, bias=False)\n",
       "          (up_proj): Linear(in_features=16, out_features=8, bias=False)\n",
       "          (down_proj): Linear(in_features=8, out_features=16, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): OlmoLayerNorm()\n",
       "        (post_attention_layernorm): OlmoLayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): OlmoLayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=16, out_features=128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "olmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012\n"
     ]
    }
   ],
   "source": [
    "# Prepare the input message\n",
    "message = [\"012\"*100]\n",
    "message[0] = message[0][:150]\n",
    "inputs = tokenizer(message, return_tensors='pt', return_token_type_ids=False)\n",
    "print(message[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[[ 2.8174,  0.8746,  1.9126,  ..., -3.5451, -3.5116, -3.5138],\n",
      "         [ 1.9858,  3.4932,  2.4783,  ..., -5.3553, -5.3961, -5.3758],\n",
      "         [ 1.6352,  2.3408,  3.2565,  ..., -4.9417, -4.9648, -4.9276],\n",
      "         ...,\n",
      "         [ 2.9838,  2.3458,  3.0099,  ..., -5.5436, -5.5877, -5.5526],\n",
      "         [ 2.1330,  3.1701,  2.9955,  ..., -5.6177, -5.6961, -5.6366],\n",
      "         [ 2.3327,  2.5139,  3.3708,  ..., -5.5826, -5.6077, -5.5646]]])\n"
     ]
    }
   ],
   "source": [
    "# Get the model's output logits\n",
    "with torch.no_grad():\n",
    "    outputs = olmo(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "print(\"Logits:\", logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 150, 128])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A bigram model (designed for evaluating against other models with KL divergence)\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "class BigramModel:\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "        # row_sum is the sum across columns for each row of freq_matrix\n",
    "        # transition_matrix is freq_matrix / row_sum\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.row_sum = np.zeros(shape=(self.dim))\n",
    "        # start with uniform distribution\n",
    "        self.transition_matrix = np.full(shape=(self.dim, self.dim), fill_value=1/self.dim)\n",
    "    \n",
    "    def update(self, prev_token, current_token):\n",
    "        # assume that the tokens are normalized to be in the range [0, dim)\n",
    "        assert prev_token < self.dim, f\"prev_token out of range. prev_token: {prev_token}, dim: {self.dim}\"\n",
    "        assert current_token < self.dim, f\"current_token out of range. current_token: {current_token}, dim: {self.dim}\"\n",
    "\n",
    "        self.transition_matrix[prev_token] *= self.row_sum[prev_token] + self.dim\n",
    "        self.transition_matrix[prev_token][current_token] += 1\n",
    "        self.row_sum[prev_token] += 1\n",
    "        self.transition_matrix[prev_token] /= self.row_sum[prev_token] + self.dim\n",
    "\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.transition_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = BigramModel(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01886792, 0.96226415, 0.01886792],\n",
       "       [0.01886792, 0.01886792, 0.96226415],\n",
       "       [0.96153846, 0.01923077, 0.01923077]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the bigram model\n",
    "\n",
    "bg_input = [int(x) for x in message[0]]\n",
    "for j in range(1, len(message[0])):\n",
    "    bg.update(bg_input[j - 1], bg_input[j])\n",
    "\n",
    "bg.get_transition_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
       "        0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
       "        0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
       "        0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
       "        0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
       "        0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
       "        0, 1, 2, 0, 1, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = inputs['input_ids'][0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4008, dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "bigram_probs = bg.get_transition_matrix()[data[-1]]\n",
    "current_logits = logits[0][-1][:3]\n",
    "q = F.log_softmax(current_logits, dim=0)\n",
    "qexp = F.softmax(current_logits, dim=0)\n",
    "p = torch.tensor(bigram_probs).to(q.device)\n",
    "# reset the model for next Markov chain instance\n",
    "F.kl_div(q, p, reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.400761408274022"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([p[i]*np.log(p[i]/qexp[i]) for i in range(len(p))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model tensor([0.1991, 0.2387, 0.5622])\n",
      "bigram tensor([0.9615, 0.0192, 0.0192], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"model\", qexp)\n",
    "print(\"bigram\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (150). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
      "         0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
      "         0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
      "         0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
      "         0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
      "         0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
      "         0, 1, 2, 0, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]])}\n",
      "tensor([[0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
      "         0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
      "         0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
      "         0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
      "         0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
      "         0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
      "         0, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "159\n",
      "Input\n",
      "01201201201201201201201201201201201201201201201201201201201201201201201201201201201201201201201201201201201201201201201201201201201201201201201201201\n",
      "Output\n",
      "1111111111\n"
     ]
    }
   ],
   "source": [
    "message = [\"012\"*50]\n",
    "message[0] = message[0][:149]\n",
    "inputs = tokenizer(message, return_tensors='pt', return_token_type_ids=False)\n",
    "print(inputs)\n",
    "response = olmo.generate(**inputs, max_new_tokens=10, do_sample=False)\n",
    "# response = olmo.generate(**inputs, max_new_tokens=10, do_sample=True, top_k=50, top_p=0.95)\n",
    "print(response)\n",
    "response_str = tokenizer.batch_decode(response, skip_special_tokens=True)[0]\n",
    "print(len(response_str))\n",
    "print(\"Input\")\n",
    "print(response_str[:len(message[0])])\n",
    "print(\"Output\")\n",
    "print(response_str[len(message[0]):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/n/home07/jchooi/OLMo2/OLMo/1-setup-mc'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2,\n",
       "        1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2,\n",
       "        1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2,\n",
       "        1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2,\n",
       "        1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2,\n",
       "        1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2,\n",
       "        1, 0, 2, 1, 0, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the target directory to the system path\n",
    "scripts_dir = '/n/home07/jchooi/OLMo2/OLMo/scripts'\n",
    "sys.path.append(scripts_dir)\n",
    "\n",
    "from markov.generate_markov_chains import generate_markov_chains\n",
    "\n",
    "chains = generate_markov_chains(num_seq=500, num_symbols=3, seq_len=150, deterministic=True)\n",
    "chains[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.156\n"
     ]
    }
   ],
   "source": [
    "sum_correct = 0\n",
    "\n",
    "for chain in chains:\n",
    "    inputs = [''.join([str(x) for x in chain[:-1].numpy()])]\n",
    "    correct_response = str(chain[-1].item())\n",
    "    inputs_tokenized = tokenizer(inputs, return_tensors='pt', return_token_type_ids=False)\n",
    "    response = olmo.generate(**inputs_tokenized, max_new_tokens=1, do_sample=False)\n",
    "    response_str = tokenizer.batch_decode(response, skip_special_tokens=True)[0][-1]\n",
    "    # print(\"Input:\", inputs)\n",
    "    # print(\"correct_response:\", correct_response)\n",
    "    # print(\"response_str:\", response_str)\n",
    "    sum_correct += (correct_response == response_str)\n",
    "\n",
    "print(sum_correct/len(chains))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try probabilistic Markov chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 1, 1, 1, 0, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 0, 2, 1, 1,\n",
       "        1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 0, 2, 2, 1, 2, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 0, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 2, 1, 2, 1, 2, 2, 1, 1, 2, 0,\n",
       "        2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2,\n",
       "        2, 1, 1, 1, 1, 1, 0, 2, 2, 0, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chains = generate_markov_chains(num_seq=500, num_symbols=3, seq_len=150, deterministic=False)\n",
    "chains[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.424\n"
     ]
    }
   ],
   "source": [
    "sum_correct = 0\n",
    "\n",
    "for chain in chains:\n",
    "    inputs = [''.join([str(x) for x in chain[:-1].numpy()])]\n",
    "    correct_response = str(chain[-1].item())\n",
    "    inputs_tokenized = tokenizer(inputs, return_tensors='pt', return_token_type_ids=False)\n",
    "    response = olmo.generate(**inputs_tokenized, max_new_tokens=1, do_sample=False)\n",
    "    response_str = tokenizer.batch_decode(response, skip_special_tokens=True)[0][-1]\n",
    "    # print(\"Input:\", inputs)\n",
    "    # print(\"correct_response:\", correct_response)\n",
    "    # print(\"response_str:\", response_str)\n",
    "    sum_correct += (correct_response == response_str)\n",
    "\n",
    "print(sum_correct/len(chains))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (olmo2)",
   "language": "python",
   "name": "olmo2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
