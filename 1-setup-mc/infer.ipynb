{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home07/jchooi/.conda/envs/olmo2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from olmo.inference import load_inferencer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set environment variables for single process\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12355'  # Use a port number that is free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiprocessing start method set to 'spawn'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W socket.cpp:464] [c10d] The server socket cannot be initialized on [::]:12355 (errno: 97 - Address family not supported by protocol).\n",
      "[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:12355 (errno: 97 - Address family not supported by protocol).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-12 12:23:07.297\tholygpu8a19101.rc.fas.harvard.edu:0\tinference:58\tINFO\tCLI environment prepared\n",
      "cfg:  TrainConfig(run_name='use-rope', seed=42, epoch=None, dry_run=False, model=ModelConfig(d_model=16, n_heads=1, n_kv_heads=None, clip_qkv=None, n_layers=2, mlp_ratio=1, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', block_group_size=1, alibi=False, alibi_bias_max=8.0, rope=True, rope_full_precision=True, flash_attention=False, attention_dropout=0.0, multi_query_attention=True, attention_layer_norm=True, residual_dropout=0.0, embedding_dropout=0.0, layer_norm_type='low_precision', layer_norm_with_affine=True, layer_norm_eps=1e-05, attention_layer_norm_with_affine=True, max_sequence_length=150, include_bias=True, bias_for_layer_norm=None, scale_logits=False, vocab_size=8, embedding_size=128, weight_tying=True, eos_token_id=6, pad_token_id=7, init_device='meta', init_fn='normal', init_std=0.02, init_cutoff_factor=None, precision='amp_bf16'), optimizer=OptimizerConfig(name='adamw', learning_rate=0.001, weight_decay=0.0, betas=(0.9, 0.95), eps=1e-05, no_decay_norm_and_bias=None, selective_updates=False, decay_norm_and_bias=False, decay_embeddings=False, metrics_log_interval=None, record_update_metrics=False), scheduler=SchedulerConfig(name='cosine_with_warmup', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=None), data=DataConfig(paths=['/n/holyscratch01/sham_lab/summer_2024/datasets/special_markov/input_ids.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=16, persistent_workers=True, timeout=0, seed=None, instance_filter=None), restore_dataloader=True, fast_forward_batches=None, evaluators=[EvaluatorConfig(label='mc-bigram-validation', type='bg', data=DataConfig(paths=['/n/holyscratch01/sham_lab/summer_2024/markov_dataset_1M.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, num_workers=1, drop_last=True, pin_memory=True, prefetch_factor=4, persistent_workers=True, timeout=0, seed=None, instance_filter=None), device_eval_batch_size=None, subset_num_batches=10), EvaluatorConfig(label='mc-unigram-validation', type='ug', data=DataConfig(paths=['/n/holyscratch01/sham_lab/summer_2024/markov_dataset_1M.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, num_workers=1, drop_last=True, pin_memory=True, prefetch_factor=4, persistent_workers=True, timeout=0, seed=None, instance_filter=None), device_eval_batch_size=None, subset_num_batches=10), EvaluatorConfig(label='mc-uniform-validation', type='uf', data=DataConfig(paths=['/n/holyscratch01/sham_lab/summer_2024/markov_dataset_1M.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, num_workers=1, drop_last=True, pin_memory=True, prefetch_factor=4, persistent_workers=True, timeout=0, seed=None, instance_filter=None), device_eval_batch_size=None, subset_num_batches=10), EvaluatorConfig(label='deter-bigram-validation', type='bg', data=DataConfig(paths=['/n/holyscratch01/sham_lab/summer_2024/datasets/deterministic_markov/input_ids.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, num_workers=1, drop_last=True, pin_memory=True, prefetch_factor=4, persistent_workers=True, timeout=0, seed=None, instance_filter=None), device_eval_batch_size=None, subset_num_batches=10), EvaluatorConfig(label='deter-unigram-validation', type='ug', data=DataConfig(paths=['/n/holyscratch01/sham_lab/summer_2024/datasets/deterministic_markov/input_ids.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, num_workers=1, drop_last=True, pin_memory=True, prefetch_factor=4, persistent_workers=True, timeout=0, seed=None, instance_filter=None), device_eval_batch_size=None, subset_num_batches=10), EvaluatorConfig(label='deter-uniform-validation', type='uf', data=DataConfig(paths=['/n/holyscratch01/sham_lab/summer_2024/datasets/deterministic_markov/input_ids.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, num_workers=1, drop_last=True, pin_memory=True, prefetch_factor=4, persistent_workers=True, timeout=0, seed=None, instance_filter=None), device_eval_batch_size=None, subset_num_batches=10)], eval_interval=250, tokenizer=TokenizerConfig(identifier='../olmo_data/tokenizers/8-token-tokenizer.json', truncate_direction='right'), save_folder='/n/holyscratch01/sham_lab/summer_2024/checkpoints/use-rope-39349756', remote_save_folder=None, canceled_check_interval=50, save_interval=1000, save_interval_unsharded=1000, save_interval_ephemeral=None, save_num_checkpoints_to_keep=9, save_num_unsharded_checkpoints_to_keep=-1, save_overwrite=True, force_save_unsharded=False, no_pre_train_checkpoint=False, load_path='/n/holyscratch01/sham_lab/summer_2024/checkpoints/use-rope-39349756/step2000-unsharded', load_path_sharded_checkpointer=None, reset_optimizer_state=False, reset_trainer_state=False, sharded_checkpointer='torch_legacy', new_style_checkpoints=None, max_duration='10ep', global_train_batch_size=1024, device_train_batch_size=1024, device_train_microbatch_size=64, device_eval_batch_size=64, eval_subset_num_batches=-1, eval_on_load=False, device_train_grad_accum=16, max_grad_norm=1.0, max_grad_norm_ratio=None, precision='amp_bf16', wandb=WandbConfig(project='mc-2', entity='jchooi', group=None, name='use-rope', tags=['watching'], log_artifacts=False, rank_zero_only=True, log_interval=1), speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=1, gen1_gc_interval=1, compile=None, distributed_strategy='fsdp', fsdp=FSDPConfig(use_orig_params=True, sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, wrapping_strategy=None, precision='pure', hybrid_sharding_num_model_replicas=None), ddp=None, softmax_auxiliary_loss=False, time_limit=None, extra_steps_after_cancel=10, early_stopping_factor=None, save_data_indices=True, python_profiling=False, torch_profiling=False, stop_at=None, stop_after=None, activation_checkpointing=None, fused_loss=None, hf_datasets_cache_dir=None)\n",
      "tokenizer:  ../olmo_data/tokenizers/8-token-tokenizer.json\n",
      "2024-07-12 12:23:07.448\tholygpu8a19101.rc.fas.harvard.edu:0\tinference:89\tINFO\tConfiguration:\n",
      "2024-07-12 12:23:07.449\tholygpu8a19101.rc.fas.harvard.edu:0\tinference:90\tINFO\tTrainConfig(run_name='use-rope', seed=42, epoch=None, dry_run=False, model=ModelConfig(d_model=16, n_heads=1, n_kv_heads=None, clip_qkv=None, n_layers=2, mlp_ratio=1, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', block_group_size=1, alibi=False, alibi_bias_max=8.0, rope=True, rope_full_precision=True, flash_attention=False, attention_dropout=0.0, multi_query_attention=True, attention_layer_norm=True, residual_dropout=0.0, embedding_dropout=0.0, layer_norm_type='low_precision', layer_norm_with_affine=True, layer_norm_eps=1e-05, attention_layer_norm_with_affine=True, max_sequence_length=150, include_bias=True, bias_for_layer_norm=None, scale_logits=False, vocab_size=8, embedding_size=128, weight_tying=True, eos_token_id=6, pad_token_id=7, init_device='meta', init_fn='normal', init_std=0.02, init_cutoff_factor=None, precision='amp_bf16'), optimizer=OptimizerConfig(name='adamw', learning_rate=0.001, weight_decay=0.0, betas=(0.9, 0.95), eps=1e-05, no_decay_norm_and_bias=None, selective_updates=False, decay_norm_and_bias=False, decay_embeddings=False, metrics_log_interval=None, record_update_metrics=False), scheduler=SchedulerConfig(name='cosine_with_warmup', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=None), data=DataConfig(paths=['/n/holyscratch01/sham_lab/summer_2024/datasets/special_markov/input_ids.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=16, persistent_workers=True, timeout=0, seed=None, instance_filter=None), restore_dataloader=True, fast_forward_batches=None, evaluators=[EvaluatorConfig(label='mc-bigram-validation', type='bg', data=DataConfig(paths=['/n/holyscratch01/sham_lab/summer_2024/markov_dataset_1M.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, num_workers=1, drop_last=True, pin_memory=True, prefetch_factor=4, persistent_workers=True, timeout=0, seed=None, instance_filter=None), device_eval_batch_size=None, subset_num_batches=10), EvaluatorConfig(label='mc-unigram-validation', type='ug', data=DataConfig(paths=['/n/holyscratch01/sham_lab/summer_2024/markov_dataset_1M.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, num_workers=1, drop_last=True, pin_memory=True, prefetch_factor=4, persistent_workers=True, timeout=0, seed=None, instance_filter=None), device_eval_batch_size=None, subset_num_batches=10), EvaluatorConfig(label='mc-uniform-validation', type='uf', data=DataConfig(paths=['/n/holyscratch01/sham_lab/summer_2024/markov_dataset_1M.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, num_workers=1, drop_last=True, pin_memory=True, prefetch_factor=4, persistent_workers=True, timeout=0, seed=None, instance_filter=None), device_eval_batch_size=None, subset_num_batches=10), EvaluatorConfig(label='deter-bigram-validation', type='bg', data=DataConfig(paths=['/n/holyscratch01/sham_lab/summer_2024/datasets/deterministic_markov/input_ids.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, num_workers=1, drop_last=True, pin_memory=True, prefetch_factor=4, persistent_workers=True, timeout=0, seed=None, instance_filter=None), device_eval_batch_size=None, subset_num_batches=10), EvaluatorConfig(label='deter-unigram-validation', type='ug', data=DataConfig(paths=['/n/holyscratch01/sham_lab/summer_2024/datasets/deterministic_markov/input_ids.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, num_workers=1, drop_last=True, pin_memory=True, prefetch_factor=4, persistent_workers=True, timeout=0, seed=None, instance_filter=None), device_eval_batch_size=None, subset_num_batches=10), EvaluatorConfig(label='deter-uniform-validation', type='uf', data=DataConfig(paths=['/n/holyscratch01/sham_lab/summer_2024/datasets/deterministic_markov/input_ids.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, num_workers=1, drop_last=True, pin_memory=True, prefetch_factor=4, persistent_workers=True, timeout=0, seed=None, instance_filter=None), device_eval_batch_size=None, subset_num_batches=10)], eval_interval=250, tokenizer=TokenizerConfig(identifier='../olmo_data/tokenizers/8-token-tokenizer.json', truncate_direction='right'), save_folder='/n/holyscratch01/sham_lab/summer_2024/checkpoints/use-rope-39349756', remote_save_folder=None, canceled_check_interval=50, save_interval=1000, save_interval_unsharded=1000, save_interval_ephemeral=None, save_num_checkpoints_to_keep=9, save_num_unsharded_checkpoints_to_keep=-1, save_overwrite=True, force_save_unsharded=False, no_pre_train_checkpoint=False, load_path='/n/holyscratch01/sham_lab/summer_2024/checkpoints/use-rope-39349756/step2000-unsharded', load_path_sharded_checkpointer=None, reset_optimizer_state=False, reset_trainer_state=False, sharded_checkpointer='torch_legacy', new_style_checkpoints=None, max_duration='10ep', global_train_batch_size=1024, device_train_batch_size=1024, device_train_microbatch_size=64, device_eval_batch_size=64, eval_subset_num_batches=-1, eval_on_load=False, device_train_grad_accum=16, max_grad_norm=1.0, max_grad_norm_ratio=None, precision='amp_bf16', wandb=WandbConfig(project='mc-2', entity='jchooi', group=None, name='use-rope', tags=['watching'], log_artifacts=False, rank_zero_only=True, log_interval=1), speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=1, gen1_gc_interval=1, compile=None, distributed_strategy='fsdp', fsdp=FSDPConfig(use_orig_params=True, sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, wrapping_strategy=None, precision='pure', hybrid_sharding_num_model_replicas=None), ddp=None, softmax_auxiliary_loss=False, time_limit=None, extra_steps_after_cancel=10, early_stopping_factor=None, save_data_indices=True, python_profiling=False, torch_profiling=False, stop_at=None, stop_after=None, activation_checkpointing=None, fused_loss=None, hf_datasets_cache_dir=None)\n",
      "2024-07-12 12:23:15.340\tholygpu8a19101.rc.fas.harvard.edu:0\tolmo.data.iterable_dataset:76\tINFO\tSaving global data order indices...\n",
      "2024-07-12 12:23:15.396\tholygpu8a19101.rc.fas.harvard.edu:0\tolmo.data.iterable_dataset:85\tINFO\tGlobal data order indices saved to '/n/holyscratch01/sham_lab/summer_2024/checkpoints/use-rope-39349756/train_data/global_indices.npy'\n",
      "2024-07-12 12:23:15.397\tholygpu8a19101.rc.fas.harvard.edu:0\tinference:111\tINFO\tBuilding model...\n",
      "2024-07-12 12:23:15.455\tholygpu8a19101.rc.fas.harvard.edu:0\tinference:113\tINFO\tTotal number of parameters: 5,344\n",
      "2024-07-12 12:23:15.455\tholygpu8a19101.rc.fas.harvard.edu:0\tinference:114\tINFO\tNumber of non-embedding parameters: 3,296\n",
      "2024-07-12 12:23:15.456\tholygpu8a19101.rc.fas.harvard.edu:0\tinference:115\tINFO\tPeak GPU Memory (MB) before fsdp: 0\n",
      "2024-07-12 12:23:15.456\tholygpu8a19101.rc.fas.harvard.edu:0\tinference:137\tINFO\tWrapping model with FSDP...\n",
      "2024-07-12 12:23:15.461\tholygpu8a19101.rc.fas.harvard.edu:0\tolmo.model:1047\tINFO\tInitializing model parameters...\n",
      "2024-07-12 12:23:15.463\tholygpu8a19101.rc.fas.harvard.edu:0\tinference:193\tINFO\tPeak GPU Memory (MB) after fsdp: 0\n",
      "2024-07-12 12:23:15.463\tholygpu8a19101.rc.fas.harvard.edu:0\tinference:194\tINFO\tModel:\n",
      "2024-07-12 12:23:15.464\tholygpu8a19101.rc.fas.harvard.edu:0\tinference:195\tINFO\tFullyShardedDataParallel(\n",
      "  (_fsdp_wrapped_module): OLMo(\n",
      "    (transformer): ModuleDict(\n",
      "      (wte): Embedding(128, 16)\n",
      "      (emb_drop): Dropout(p=0.0, inplace=False)\n",
      "      (ln_f): LayerNorm()\n",
      "      (blocks): ModuleList(\n",
      "        (0-1): 2 x OLMoSequentialBlock(\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_norm): LayerNorm()\n",
      "          (q_norm): LayerNorm()\n",
      "          (act): SwiGLU()\n",
      "          (attn_out): Linear(in_features=16, out_features=16, bias=True)\n",
      "          (ff_out): Linear(in_features=8, out_features=16, bias=True)\n",
      "          (rotary_emb): RotaryEmbedding()\n",
      "          (attn_norm): LayerNorm()\n",
      "          (ff_norm): LayerNorm()\n",
      "          (att_proj): Linear(in_features=16, out_features=48, bias=True)\n",
      "          (ff_proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2024-07-12 12:23:15.464\tholygpu8a19101.rc.fas.harvard.edu:0\tolmo.optim:909\tINFO\tConstructing optimizer with 2 param groups\n",
      "Loading checkpoint from /n/holyscratch01/sham_lab/summer_2024/checkpoints/use-rope-39349756/step2000-unsharded...\n",
      "2024-07-12 12:23:15.804\tholygpu8a19101.rc.fas.harvard.edu:0\tolmo.checkpoint:753\tINFO\tLoading optimizer state turn 0 ...\n",
      "2024-07-12 12:23:15.804\tholygpu8a19101.rc.fas.harvard.edu:0\tolmo.checkpoint:219\tINFO\tFlattening sharded optimizer state...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[2024-07-12 12:23:15,807] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _flatten_optim_state_dict() profiling:  defaultdict(<class 'float'>, {<Type.RESHARDING: 'resharding'>: 0.0002891991753131151})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-12 12:23:15.937\tholygpu8a19101.rc.fas.harvard.edu:0\tolmo.checkpoint:233\tINFO\tLoading flattened optimizer state...\n",
      "2024-07-12 12:23:16.205\tholygpu8a19101.rc.fas.harvard.edu:0\tolmo.inferencer:376\tINFO\tData loader will start at instance index 49,152\n",
      "2024-07-12 12:23:16.205\tholygpu8a19101.rc.fas.harvard.edu:0\tolmo.inferencer:380\tINFO\tResetting learning rate...\n",
      "2024-07-12 12:23:16.206\tholygpu8a19101.rc.fas.harvard.edu:0\tolmo.inferencer:392\tINFO\tRestoring RNG states...\n",
      "Checkpoint successfully loaded\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"/n/holyscratch01/sham_lab/summer_2024/checkpoints/use-rope-39349756/step2000-unsharded\"\n",
    "\n",
    "olmo = load_inferencer(checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inferencer(cfg=TrainConfig(run_name='use-rope', seed=42, epoch=None, dry_run=False, model=ModelConfig(d_model=16, n_heads=1, n_kv_heads=None, clip_qkv=None, n_layers=2, mlp_ratio=1, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', block_group_size=1, alibi=False, alibi_bias_max=8.0, rope=True, rope_full_precision=True, flash_attention=False, attention_dropout=0.0, multi_query_attention=True, attention_layer_norm=True, residual_dropout=0.0, embedding_dropout=0.0, layer_norm_type='low_precision', layer_norm_with_affine=True, layer_norm_eps=1e-05, attention_layer_norm_with_affine=True, max_sequence_length=150, include_bias=True, bias_for_layer_norm=None, scale_logits=False, vocab_size=8, embedding_size=128, weight_tying=True, eos_token_id=6, pad_token_id=7, init_device='meta', init_fn='normal', init_std=0.02, init_cutoff_factor=None, precision='amp_bf16'), optimizer=OptimizerConfig(name='adamw', learning_rate=0.001, weight_decay=0.0, betas=(0.9, 0.95), eps=1e-05, no_decay_norm_and_bias=None, selective_updates=False, decay_norm_and_bias=False, decay_embeddings=False, metrics_log_interval=None, record_update_metrics=False), scheduler=SchedulerConfig(name='cosine_with_warmup', units='steps', t_warmup=100, t_max=None, alpha_f=0.1, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=None), data=DataConfig(paths=['/n/holyscratch01/sham_lab/summer_2024/datasets/special_markov/input_ids.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, num_workers=2, drop_last=True, pin_memory=True, prefetch_factor=16, persistent_workers=True, timeout=0, seed=None, instance_filter=None), restore_dataloader=True, fast_forward_batches=None, evaluators=[EvaluatorConfig(label='mc-bigram-validation', type='bg', data=DataConfig(paths=['/n/holyscratch01/sham_lab/summer_2024/markov_dataset_1M.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, num_workers=1, drop_last=True, pin_memory=True, prefetch_factor=4, persistent_workers=True, timeout=0, seed=None, instance_filter=None), device_eval_batch_size=None, subset_num_batches=10), EvaluatorConfig(label='mc-unigram-validation', type='ug', data=DataConfig(paths=['/n/holyscratch01/sham_lab/summer_2024/markov_dataset_1M.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, num_workers=1, drop_last=True, pin_memory=True, prefetch_factor=4, persistent_workers=True, timeout=0, seed=None, instance_filter=None), device_eval_batch_size=None, subset_num_batches=10), EvaluatorConfig(label='mc-uniform-validation', type='uf', data=DataConfig(paths=['/n/holyscratch01/sham_lab/summer_2024/markov_dataset_1M.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, num_workers=1, drop_last=True, pin_memory=True, prefetch_factor=4, persistent_workers=True, timeout=0, seed=None, instance_filter=None), device_eval_batch_size=None, subset_num_batches=10), EvaluatorConfig(label='deter-bigram-validation', type='bg', data=DataConfig(paths=['/n/holyscratch01/sham_lab/summer_2024/datasets/deterministic_markov/input_ids.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, num_workers=1, drop_last=True, pin_memory=True, prefetch_factor=4, persistent_workers=True, timeout=0, seed=None, instance_filter=None), device_eval_batch_size=None, subset_num_batches=10), EvaluatorConfig(label='deter-unigram-validation', type='ug', data=DataConfig(paths=['/n/holyscratch01/sham_lab/summer_2024/datasets/deterministic_markov/input_ids.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, num_workers=1, drop_last=True, pin_memory=True, prefetch_factor=4, persistent_workers=True, timeout=0, seed=None, instance_filter=None), device_eval_batch_size=None, subset_num_batches=10), EvaluatorConfig(label='deter-uniform-validation', type='uf', data=DataConfig(paths=['/n/holyscratch01/sham_lab/summer_2024/datasets/deterministic_markov/input_ids.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, num_workers=1, drop_last=True, pin_memory=True, prefetch_factor=4, persistent_workers=True, timeout=0, seed=None, instance_filter=None), device_eval_batch_size=None, subset_num_batches=10)], eval_interval=250, tokenizer=TokenizerConfig(identifier='../olmo_data/tokenizers/8-token-tokenizer.json', truncate_direction='right'), save_folder='/n/holyscratch01/sham_lab/summer_2024/checkpoints/use-rope-39349756', remote_save_folder=None, canceled_check_interval=50, save_interval=1000, save_interval_unsharded=1000, save_interval_ephemeral=None, save_num_checkpoints_to_keep=9, save_num_unsharded_checkpoints_to_keep=-1, save_overwrite=True, force_save_unsharded=False, no_pre_train_checkpoint=False, load_path='/n/holyscratch01/sham_lab/summer_2024/checkpoints/use-rope-39349756/step2000-unsharded', load_path_sharded_checkpointer=None, reset_optimizer_state=False, reset_trainer_state=False, sharded_checkpointer='torch_legacy', new_style_checkpoints=None, max_duration='10ep', global_train_batch_size=1024, device_train_batch_size=1024, device_train_microbatch_size=64, device_eval_batch_size=64, eval_subset_num_batches=-1, eval_on_load=False, device_train_grad_accum=16, max_grad_norm=1.0, max_grad_norm_ratio=None, precision='amp_bf16', wandb=WandbConfig(project='mc-2', entity='jchooi', group=None, name='use-rope', tags=['watching'], log_artifacts=False, rank_zero_only=True, log_interval=1), speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=1, gen1_gc_interval=1, compile=None, distributed_strategy='fsdp', fsdp=FSDPConfig(use_orig_params=True, sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, wrapping_strategy=None, precision='pure', hybrid_sharding_num_model_replicas=None), ddp=None, softmax_auxiliary_loss=False, time_limit=None, extra_steps_after_cancel=10, early_stopping_factor=None, save_data_indices=True, python_profiling=False, torch_profiling=False, stop_at=None, stop_after=None, activation_checkpointing=None, fused_loss=None, hf_datasets_cache_dir=None), model=OLMo(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(128, 16)\n",
       "    (emb_drop): Dropout(p=0.0, inplace=False)\n",
       "    (ln_f): LayerNorm()\n",
       "    (blocks): ModuleList(\n",
       "      (0-1): 2 x OLMoSequentialBlock(\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_norm): LayerNorm()\n",
       "        (q_norm): LayerNorm()\n",
       "        (act): SwiGLU()\n",
       "        (attn_out): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (ff_out): Linear(in_features=8, out_features=16, bias=True)\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (attn_norm): LayerNorm()\n",
       "        (ff_norm): LayerNorm()\n",
       "        (att_proj): Linear(in_features=16, out_features=48, bias=True)\n",
       "        (ff_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), dist_model=FullyShardedDataParallel(\n",
       "  (_fsdp_wrapped_module): OLMo(\n",
       "    (transformer): ModuleDict(\n",
       "      (wte): Embedding(128, 16)\n",
       "      (emb_drop): Dropout(p=0.0, inplace=False)\n",
       "      (ln_f): LayerNorm()\n",
       "      (blocks): ModuleList(\n",
       "        (0-1): 2 x OLMoSequentialBlock(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_norm): LayerNorm()\n",
       "          (q_norm): LayerNorm()\n",
       "          (act): SwiGLU()\n",
       "          (attn_out): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (ff_out): Linear(in_features=8, out_features=16, bias=True)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (attn_norm): LayerNorm()\n",
       "          (ff_norm): LayerNorm()\n",
       "          (att_proj): Linear(in_features=16, out_features=48, bias=True)\n",
       "          (ff_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), optim=AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.95)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-05\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    initial_lr: 0.001\n",
       "    lr: 0.0009167905520335429\n",
       "    max_grad_norm: 1.0\n",
       "    max_grad_norm_ratio: None\n",
       "    maximize: False\n",
       "    param_names: ['_fsdp_wrapped_module.transformer.blocks.0.att_proj.weight', '_fsdp_wrapped_module.transformer.blocks.0.attn_out.weight', '_fsdp_wrapped_module.transformer.blocks.0.ff_out.weight', '_fsdp_wrapped_module.transformer.blocks.0.ff_proj.weight', '_fsdp_wrapped_module.transformer.blocks.1.att_proj.weight', '_fsdp_wrapped_module.transformer.blocks.1.attn_out.weight', '_fsdp_wrapped_module.transformer.blocks.1.ff_out.weight', '_fsdp_wrapped_module.transformer.blocks.1.ff_proj.weight']\n",
       "    sharded: True\n",
       "    weight_decay: 0.0\n",
       "\n",
       "Parameter Group 1\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.95)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-05\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    initial_lr: 0.001\n",
       "    lr: 0.0009167905520335429\n",
       "    max_grad_norm: 1.0\n",
       "    max_grad_norm_ratio: None\n",
       "    maximize: False\n",
       "    param_names: ['_fsdp_wrapped_module.transformer.blocks.0.att_proj.bias', '_fsdp_wrapped_module.transformer.blocks.0.attn_norm.bias', '_fsdp_wrapped_module.transformer.blocks.0.attn_norm.weight', '_fsdp_wrapped_module.transformer.blocks.0.attn_out.bias', '_fsdp_wrapped_module.transformer.blocks.0.ff_norm.bias', '_fsdp_wrapped_module.transformer.blocks.0.ff_norm.weight', '_fsdp_wrapped_module.transformer.blocks.0.ff_out.bias', '_fsdp_wrapped_module.transformer.blocks.0.ff_proj.bias', '_fsdp_wrapped_module.transformer.blocks.0.k_norm.bias', '_fsdp_wrapped_module.transformer.blocks.0.k_norm.weight', '_fsdp_wrapped_module.transformer.blocks.0.q_norm.bias', '_fsdp_wrapped_module.transformer.blocks.0.q_norm.weight', '_fsdp_wrapped_module.transformer.blocks.1.att_proj.bias', '_fsdp_wrapped_module.transformer.blocks.1.attn_norm.bias', '_fsdp_wrapped_module.transformer.blocks.1.attn_norm.weight', '_fsdp_wrapped_module.transformer.blocks.1.attn_out.bias', '_fsdp_wrapped_module.transformer.blocks.1.ff_norm.bias', '_fsdp_wrapped_module.transformer.blocks.1.ff_norm.weight', '_fsdp_wrapped_module.transformer.blocks.1.ff_out.bias', '_fsdp_wrapped_module.transformer.blocks.1.ff_proj.bias', '_fsdp_wrapped_module.transformer.blocks.1.k_norm.bias', '_fsdp_wrapped_module.transformer.blocks.1.k_norm.weight', '_fsdp_wrapped_module.transformer.blocks.1.q_norm.bias', '_fsdp_wrapped_module.transformer.blocks.1.q_norm.weight', '_fsdp_wrapped_module.transformer.ln_f.bias', '_fsdp_wrapped_module.transformer.ln_f.weight', '_fsdp_wrapped_module.transformer.wte.weight']\n",
       "    sharded: True\n",
       "    weight_decay: 0.0\n",
       "), scheduler=CosWithWarmup(grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=None, warmup_steps=100, alpha_f=0.1, t_max=None), train_loader=<torch.utils.data.dataloader.DataLoader object at 0x147ac1dda890>, device=device(type='cuda'), epoch=2, global_step=2000, global_train_examples_seen_this_epoch=49152, global_train_tokens_seen=307200000, checkpoints=[PosixPath('/n/holyscratch01/sham_lab/summer_2024/checkpoints/use-rope-39349756/step0'), PosixPath('/n/holyscratch01/sham_lab/summer_2024/checkpoints/use-rope-39349756/step1000'), PosixPath('/n/holyscratch01/sham_lab/summer_2024/checkpoints/use-rope-39349756/step2000')], unsharded_checkpoints=[PosixPath('/n/holyscratch01/sham_lab/summer_2024/checkpoints/use-rope-39349756/step1000-unsharded'), PosixPath('/n/holyscratch01/sham_lab/summer_2024/checkpoints/use-rope-39349756/step2000-unsharded')], ephemeral_checkpoints=[], min_train_loss=inf, cur_train_loss=inf, indices_file=None, _start_time=0.0, _gc_init_state=True, loss_fn=<function cross_entropy_loss at 0x147ace417380>, last_sharded_checkpoint_step=None, last_unsharded_checkpoint_step=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "olmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../olmo_data/tokenizers/8-token-tokenizer.json'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "olmo.cfg.tokenizer.identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/n/holyscratch01/sham_lab/summer_2024/checkpoints/use-rope-39349756/step3000-unsharded-hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
      "         0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
      "         0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
      "         0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
      "         0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
      "         0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
      "         0, 1, 2, 0, 1, 2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Prepare the input message\n",
    "message = [\"012\"*100]\n",
    "message[0] = message[0][:150]\n",
    "inputs = tokenizer(message, return_tensors='pt', return_token_type_ids=False)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
       "         0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
       "         0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
       "         0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
       "         0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
       "         0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,\n",
       "         0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "olmo.generate(inputs, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "next_token_ids tensor([1], device='cuda:0')\n",
      "next_token_ids tensor([2], device='cuda:0')\n",
      "next_token_ids tensor([0], device='cuda:0')\n",
      "Input\n",
      "012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012012\n",
      "Output\n",
      "0120120120120120120120120120120120120120120120120120120120120120120120120120120120120120120120120120\n"
     ]
    }
   ],
   "source": [
    "response = olmo.generate(inputs, do_sample=False, max_new_tokens=100)[0]\n",
    "response_str = tokenizer.decode(response, skip_special_tokens=True)\n",
    "print(\"Input\")\n",
    "print(response_str[:inputs['input_ids'].shape[1]])\n",
    "print(\"Output\")\n",
    "print(response_str[inputs['input_ids'].shape[1]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (olmo2)",
   "language": "python",
   "name": "olmo2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
