{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook processes the TinyStories dataset so that it only has lower case alphabets, space, commas and periods.\n",
    "\n",
    "Input:\n",
    "`/n/netscratch/sham_lab/Everyone/jchooi/in-context-language-learning/data/TinyStoriesV2-GPT4-train.txt`\n",
    "\n",
    "Output:\n",
    "`/n/netscratch/sham_lab/Everyone/jchooi/in-context-language-learning/data/TinyStories-processed.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2124/2124 [01:12<00:00, 29.47MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Character      Count\n",
      "0          \\n   15600056\n",
      "1           O    4637239\n",
      "2           n  102941792\n",
      "3           c   30276870\n",
      "4           e  209712524\n",
      "..        ...        ...\n",
      "223         œ          5\n",
      "224         ê          2\n",
      "225                    2\n",
      "226         «          5\n",
      "227         »          5\n",
      "\n",
      "[228 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_file = '/n/netscratch/sham_lab/Everyone/jchooi/in-context-language-learning/data/TinyStoriesV2-GPT4-train.txt'\n",
    "\n",
    "# Initialize a counter\n",
    "char_counter = Counter()\n",
    "\n",
    "# Get the file size\n",
    "file_size = os.path.getsize(input_file)\n",
    "\n",
    "# Read the file in chunks to handle large size with a progress bar\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    for chunk in tqdm(iter(lambda: file.read(1024 * 1024), ''), total=file_size // (1024 * 1024), unit='MB'):\n",
    "        char_counter.update(chunk)\n",
    "\n",
    "# Convert the counter to a DataFrame for better visualization\n",
    "char_count_df = pd.DataFrame(char_counter.items(), columns=['Character', 'Count'])\n",
    "print(char_count_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_count_df = char_count_df.sort_values(by=\"Count\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_count_df.to_csv(\"temp/char_count_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbols related to eos\n",
    "# |,5435403\n",
    "# <,2717731\n",
    "# >,2717728"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  98%|█████████▊| 2.04G/2.07G [00:08<00:00, 246MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the output file path\n",
    "output_file_1 = '/n/netscratch/sham_lab/Everyone/jchooi/in-context-language-learning/data/TinyStories-processed-temp-1.txt'\n",
    "\n",
    "eos_token_sentence = \"<|endoftext|>\\n\"\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "     open(output_file_1, 'w', encoding='utf-8') as outfile, \\\n",
    "     tqdm(total=file_size, unit='B', unit_scale=True, unit_divisor=1024, desc='Processing') as pbar:\n",
    "    for line in infile:\n",
    "        if line == eos_token_sentence:\n",
    "            continue\n",
    "        outfile.write(line)\n",
    "        pbar.update(len(line.encode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2088/2088 [01:11<00:00, 29.13MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a counter\n",
    "char_counter2 = Counter()\n",
    "\n",
    "output_file_size = os.path.getsize(output_file_1)\n",
    "\n",
    "# Read the file in chunks to handle large size with a progress bar\n",
    "with open(output_file_1, 'r', encoding='utf-8') as file:\n",
    "    for chunk in tqdm(iter(lambda: file.read(1024 * 1024), ''), total=output_file_size // (1024 * 1024), unit='MB'):\n",
    "        char_counter2.update(chunk)\n",
    "\n",
    "# Convert the counter to a DataFrame for better visualization\n",
    "char_count2_df = pd.DataFrame(char_counter2.items(), columns=['Character', 'Count'])\n",
    "char_count2_df = char_count2_df.sort_values(by=\"Count\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_count2_df.to_csv(\"temp/char_count_df-2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace fancy quotes with normal quotes\n",
    "def replace_fancy_quotes(text):\n",
    "    text = text.replace('“', '\"')\n",
    "    text = text.replace('”', '\"')\n",
    "    text = text.replace('’', '\\'')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define a function to check if a sentence contains only alphabets or punctuation\n",
    "def is_valid_line(line):\n",
    "    return re.match(r'^[a-zA-Z .,]*\\n$', line) is not None\n",
    "\n",
    "output_file_2 = '/n/netscratch/sham_lab/Everyone/jchooi/in-context-language-learning/data/TinyStories-processed-temp-2.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████▉| 2.04G/2.04G [00:55<00:00, 39.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "file_size = os.path.getsize(output_file_1)\n",
    "\n",
    "char_counter3 = Counter()\n",
    "\n",
    "# Read the processed file and filter sentences\n",
    "with open(output_file_1, 'r', encoding='utf-8') as infile, \\\n",
    "     open(output_file_2, 'w', encoding='utf-8') as outfile, \\\n",
    "        tqdm(total=file_size, unit='B', unit_scale=True, unit_divisor=1024, desc='Processing') as pbar:\n",
    "    for line in infile:\n",
    "        line = replace_fancy_quotes(line)\n",
    "        if is_valid_line(line):\n",
    "            outfile.write(line)\n",
    "            char_counter3.update(line)\n",
    "        pbar.update(len(line.encode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_count3_df = pd.DataFrame(char_counter3.items(), columns=['Character', 'Count'])\n",
    "char_count3_df = char_count3_df.sort_values(by=\"Count\",ascending=False)\n",
    "char_count3_df.to_csv(\"temp/char_count_df-3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 967M/967M [00:19<00:00, 52.0MB/s] \n"
     ]
    }
   ],
   "source": [
    "# now we remove all newlines\n",
    "# and treat each sentence as its own\n",
    "# so each sentense is on its own line\n",
    "# and make it lowercase\n",
    "# make commas and periods as their own word\n",
    "# so we space them out\n",
    "\n",
    "output_file_3 = '/n/netscratch/sham_lab/Everyone/jchooi/in-context-language-learning/data/TinyStories-processed.txt'\n",
    "\n",
    "file_size = os.path.getsize(output_file_2)\n",
    "\n",
    "char_counter3 = Counter()\n",
    "\n",
    "def space_out_commas(sentence):\n",
    "    sentence = ' , '.join([chunk.strip() for chunk in sentence.split(\",\")])\n",
    "    return sentence\n",
    "\n",
    "# Read the processed file and filter sentences\n",
    "with open(output_file_2, 'r', encoding='utf-8') as infile, \\\n",
    "     open(output_file_3, 'w', encoding='utf-8') as outfile, \\\n",
    "        tqdm(total=file_size, unit='B', unit_scale=True, unit_divisor=1024, desc='Processing') as pbar:\n",
    "    for line in infile:\n",
    "        line = line.lower()\n",
    "        lines = ''.join([space_out_commas(sentence.strip()) + \" .\\n\" for sentence in line.split(\".\") if sentence.strip() != ''])\n",
    "        outfile.write(lines)\n",
    "        pbar.update(len(line.encode('utf-8')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
