{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves as a playground to test out the pomegranate library. The goal is to fit a simple HMM given known data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pomegranate.hmm import DenseHMM\n",
    "from pomegranate.distributions import Categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 100, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate random data\n",
    "# first generate the hidden states\n",
    "# then generate the observations\n",
    "\n",
    "def generate_next_hidden_state(current_state: int):\n",
    "    if current_state == 0:\n",
    "        # switch 90% of the time\n",
    "        return 1 if np.random.random(size=(1)) < 0.9 else 0\n",
    "    else:\n",
    "        # switch 50% of the time\n",
    "        return 1 if np.random.random(size=(1)) < 0.5 else 0\n",
    "\n",
    "def generate_hidden_states(batch_size:int, context_length: int):\n",
    "    hidden_sequences = []\n",
    "    for _ in range(batch_size):\n",
    "        # get the starting state which is 0 or 1\n",
    "        hidden_sequence = [np.random.randint(low=0,high=2)]\n",
    "        while len(hidden_sequence) < context_length:\n",
    "            hidden_sequence.append(generate_next_hidden_state(hidden_sequence[-1]))\n",
    "        hidden_sequences.append(hidden_sequence)\n",
    "    hidden_sequences = np.array(hidden_sequences).reshape(batch_size, context_length, 1)\n",
    "    return hidden_sequences\n",
    "\n",
    "batch_size = 200\n",
    "context_length = 100\n",
    "hidden_sequences = generate_hidden_states(batch_size=batch_size, context_length=context_length)\n",
    "hidden_sequences.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 100, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we generate the emissions\n",
    "def _get_emission(hidden_state):\n",
    "    if hidden_state == 0:\n",
    "        # emit 0 or 1 with 50% each\n",
    "        return 1 if np.random.random(size=(1)) < 0.5 else 0\n",
    "    else:\n",
    "        # emit 0 80% of the time, and 1 20% of the time\n",
    "        return 0 if np.random.random(size=(1)) < 0.8 else 1\n",
    "\n",
    "# vectorize it\n",
    "get_emission = np.vectorize(_get_emission)\n",
    "emission_sequences = get_emission(hidden_sequences)\n",
    "emission_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Improvement: -1925.69140625, Time: 0.02726s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenseHMM(\n",
       "  (start): Silent()\n",
       "  (end): Silent()\n",
       "  (distributions): ModuleList(\n",
       "    (0-1): 2 x Categorical()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the HMM\n",
    "d1 = Categorical([[0.5,0.5]])\n",
    "d2 = Categorical([[0.8,0.2]])\n",
    "\n",
    "model = DenseHMM(distributions=[d1, d2], verbose=True)\n",
    "model.fit(emission_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have a working model with non-nan improvement. Let's see if we can find its learnt transition matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4820, 0.5130],\n",
       "        [0.4852, 0.5098]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(model.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The emission matrix is wrong. It is 50% for each emission above, but by design we have 50%/50% for state 0 and 80%/20% for state 1.\n",
    "\n",
    "Let's increase the training dataset size and see if the model learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 400\n",
    "context_length = 400\n",
    "hidden_sequences = generate_hidden_states(batch_size=batch_size, context_length=context_length)\n",
    "emission_sequences = get_emission(hidden_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Improvement: -1890.4765625, Time: 0.05093s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenseHMM(\n",
       "  (start): Silent()\n",
       "  (end): Silent()\n",
       "  (distributions): ModuleList(\n",
       "    (0-1): 2 x Categorical()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the HMM\n",
    "d1 = Categorical([[0.5,0.5]])\n",
    "d2 = Categorical([[0.8,0.2]])\n",
    "\n",
    "model = DenseHMM(distributions=[d1, d2], verbose=True)\n",
    "model.fit(emission_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4833, 0.5143],\n",
       "        [0.4866, 0.5108]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(model.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model doesn't learn. Let's try with a different starting emission distribution for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Improvement: 406.96875, Time: 0.05005s\n",
      "[2] Improvement: 19.640625, Time: 0.05073s\n",
      "[3] Improvement: 15.171875, Time: 0.05011s\n",
      "[4] Improvement: 11.7421875, Time: 0.04996s\n",
      "[5] Improvement: 10.0859375, Time: 0.05115s\n",
      "[6] Improvement: 7.2421875, Time: 0.05062s\n",
      "[7] Improvement: 5.8828125, Time: 0.05063s\n",
      "[8] Improvement: 4.9765625, Time: 0.05076s\n",
      "[9] Improvement: 3.3125, Time: 0.05125s\n",
      "[10] Improvement: 3.3828125, Time: 0.05061s\n",
      "[11] Improvement: 2.421875, Time: 0.04969s\n",
      "[12] Improvement: 1.3828125, Time: 0.04975s\n",
      "[13] Improvement: 1.5703125, Time: 0.04993s\n",
      "[14] Improvement: 1.1328125, Time: 0.05014s\n",
      "[15] Improvement: 0.5234375, Time: 0.05022s\n",
      "[16] Improvement: 0.453125, Time: 0.05074s\n",
      "[17] Improvement: 1.015625, Time: 0.04982s\n",
      "[18] Improvement: -0.1015625, Time: 0.04975s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenseHMM(\n",
       "  (start): Silent()\n",
       "  (end): Silent()\n",
       "  (distributions): ModuleList(\n",
       "    (0-1): 2 x Categorical()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the HMM\n",
    "d1 = Categorical([[0.3,0.7]])\n",
    "d2 = Categorical([[0.3,0.7]])\n",
    "\n",
    "model = DenseHMM(distributions=[d1, d2], verbose=True)\n",
    "model.fit(emission_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.3862, 0.6138]])\n",
      "Parameter containing:\n",
      "tensor([[0.9336, 0.0664]])\n"
     ]
    }
   ],
   "source": [
    "# emission probabilities\n",
    "for dist in model.distributions:\n",
    "    print(dist.probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3860, 0.6120],\n",
       "        [0.4835, 0.5136]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transition probabilities\n",
    "np.exp(model.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more than one iteration, but the transition probabilities are still wrong.\n",
    "\n",
    "Let's look at the training data and sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_sequences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is ok. Let's try another initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Improvement: 406.96875, Time: 0.05036s\n",
      "[2] Improvement: 19.640625, Time: 0.04996s\n",
      "[3] Improvement: 15.171875, Time: 0.05054s\n",
      "[4] Improvement: 11.7421875, Time: 0.05298s\n",
      "[5] Improvement: 10.0859375, Time: 0.05017s\n",
      "[6] Improvement: 7.2421875, Time: 0.05205s\n",
      "[7] Improvement: 5.8828125, Time: 0.05024s\n",
      "[8] Improvement: 4.9765625, Time: 0.0508s\n",
      "[9] Improvement: 3.3125, Time: 0.0505s\n",
      "[10] Improvement: 3.3828125, Time: 0.05158s\n",
      "[11] Improvement: 2.421875, Time: 0.05171s\n",
      "[12] Improvement: 1.3828125, Time: 0.05274s\n",
      "[13] Improvement: 1.5703125, Time: 0.05044s\n",
      "[14] Improvement: 1.1328125, Time: 0.05098s\n",
      "[15] Improvement: 0.5234375, Time: 0.05115s\n",
      "[16] Improvement: 0.453125, Time: 0.05155s\n",
      "[17] Improvement: 1.015625, Time: 0.05075s\n",
      "[18] Improvement: -0.1015625, Time: 0.04987s\n",
      "emission probabilities\n",
      "Parameter containing:\n",
      "tensor([[0.3862, 0.6138]])\n",
      "Parameter containing:\n",
      "tensor([[0.9336, 0.0664]])\n",
      "transition probabilities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3860, 0.6120],\n",
       "        [0.4835, 0.5136]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the HMM\n",
    "d1 = Categorical([[0.3,0.7]])\n",
    "d2 = Categorical([[0.9,0.1]])\n",
    "\n",
    "model = DenseHMM(distributions=[d1, d2], verbose=True)\n",
    "model.fit(emission_sequences)\n",
    "\n",
    "print(\"emission probabilities\")\n",
    "for dist in model.distributions:\n",
    "    print(dist.probs)\n",
    "\n",
    "print(\"transition probabilities\")\n",
    "np.exp(model.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's different. Let's use more training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Improvement: 1207.828125, Time: 0.09219s\n",
      "[2] Improvement: 53.28125, Time: 0.09311s\n",
      "[3] Improvement: 43.21875, Time: 0.09173s\n",
      "[4] Improvement: 33.28125, Time: 0.08891s\n",
      "[5] Improvement: 27.5, Time: 0.08947s\n",
      "[6] Improvement: 21.9375, Time: 0.08871s\n",
      "[7] Improvement: 16.125, Time: 0.08975s\n",
      "[8] Improvement: 14.015625, Time: 0.09058s\n",
      "[9] Improvement: 10.53125, Time: 0.09092s\n",
      "[10] Improvement: 7.421875, Time: 0.08927s\n",
      "[11] Improvement: 7.65625, Time: 0.09153s\n",
      "[12] Improvement: 5.46875, Time: 0.09055s\n",
      "[13] Improvement: 4.03125, Time: 0.09012s\n",
      "[14] Improvement: 2.046875, Time: 0.08996s\n",
      "[15] Improvement: 3.390625, Time: 0.08982s\n",
      "[16] Improvement: 0.9375, Time: 0.09172s\n",
      "[17] Improvement: 1.0, Time: 0.09116s\n",
      "[18] Improvement: 2.5, Time: 0.08865s\n",
      "[19] Improvement: -0.3125, Time: 0.08573s\n",
      "emission probabilities\n",
      "Parameter containing:\n",
      "tensor([[0.3868, 0.6132]])\n",
      "Parameter containing:\n",
      "tensor([[0.9343, 0.0657]])\n",
      "transition probabilities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3806, 0.6173],\n",
       "        [0.4862, 0.5111]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "context_length = 400\n",
    "hidden_sequences = generate_hidden_states(batch_size=batch_size, context_length=context_length)\n",
    "emission_sequences = get_emission(hidden_sequences)\n",
    "\n",
    "# fit the HMM\n",
    "d1 = Categorical([[0.3,0.7]])\n",
    "d2 = Categorical([[0.9,0.1]])\n",
    "\n",
    "model = DenseHMM(distributions=[d1, d2], verbose=True)\n",
    "model.fit(emission_sequences)\n",
    "\n",
    "print(\"emission probabilities\")\n",
    "for dist in model.distributions:\n",
    "    print(dist.probs)\n",
    "\n",
    "print(\"transition probabilities\")\n",
    "np.exp(model.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Improvement: 11882.0, Time: 0.7049s\n",
      "[2] Improvement: 544.75, Time: 0.7117s\n",
      "[3] Improvement: 408.25, Time: 0.718s\n",
      "[4] Improvement: 342.75, Time: 0.717s\n",
      "[5] Improvement: 279.25, Time: 0.7203s\n",
      "[6] Improvement: 199.75, Time: 0.7041s\n",
      "[7] Improvement: 163.0, Time: 0.7048s\n",
      "[8] Improvement: 139.0, Time: 0.7146s\n",
      "[9] Improvement: 105.5, Time: 0.7269s\n",
      "[10] Improvement: 95.25, Time: 0.7192s\n",
      "[11] Improvement: 54.0, Time: 0.7321s\n",
      "[12] Improvement: 50.5, Time: 0.7039s\n",
      "[13] Improvement: 47.25, Time: 0.7103s\n",
      "[14] Improvement: 18.25, Time: 0.7208s\n",
      "[15] Improvement: 33.75, Time: 0.7167s\n",
      "[16] Improvement: 9.75, Time: 0.7289s\n",
      "[17] Improvement: 18.5, Time: 0.7265s\n",
      "[18] Improvement: 12.25, Time: 0.7148s\n",
      "[19] Improvement: 10.25, Time: 0.7203s\n",
      "[20] Improvement: 2.5, Time: 0.7185s\n",
      "[21] Improvement: 9.5, Time: 0.7238s\n",
      "[22] Improvement: 5.25, Time: 0.699s\n",
      "[23] Improvement: 3.25, Time: 0.7167s\n",
      "[24] Improvement: 4.5, Time: 0.7163s\n",
      "[25] Improvement: -4.0, Time: 0.7177s\n",
      "emission probabilities\n",
      "Parameter containing:\n",
      "tensor([[0.3864, 0.6136]])\n",
      "Parameter containing:\n",
      "tensor([[0.9345, 0.0655]])\n",
      "transition probabilities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3775, 0.6200],\n",
       "        [0.4885, 0.5090]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10000\n",
    "context_length = 400\n",
    "hidden_sequences = generate_hidden_states(batch_size=batch_size, context_length=context_length)\n",
    "emission_sequences = get_emission(hidden_sequences)\n",
    "\n",
    "# fit the HMM\n",
    "d1 = Categorical([[0.3,0.7]])\n",
    "d2 = Categorical([[0.9,0.1]])\n",
    "\n",
    "model = DenseHMM(distributions=[d1, d2], verbose=True)\n",
    "model.fit(emission_sequences)\n",
    "\n",
    "print(\"emission probabilities\")\n",
    "for dist in model.distributions:\n",
    "    print(dist.probs)\n",
    "\n",
    "print(\"transition probabilities\")\n",
    "np.exp(model.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Improvement: 57394.5, Time: 0.9694s\n",
      "[2] Improvement: 654.75, Time: 0.9465s\n",
      "[3] Improvement: 517.5, Time: 0.9557s\n",
      "[4] Improvement: 404.75, Time: 0.9587s\n",
      "[5] Improvement: 349.75, Time: 0.9344s\n",
      "[6] Improvement: 254.5, Time: 0.9794s\n",
      "[7] Improvement: 224.25, Time: 0.9266s\n",
      "[8] Improvement: 166.5, Time: 0.9453s\n",
      "[9] Improvement: 119.25, Time: 0.9318s\n",
      "[10] Improvement: 89.5, Time: 0.9265s\n",
      "[11] Improvement: 86.75, Time: 0.9282s\n",
      "[12] Improvement: 75.5, Time: 0.9342s\n",
      "[13] Improvement: 40.0, Time: 0.9567s\n",
      "[14] Improvement: 47.0, Time: 0.9778s\n",
      "[15] Improvement: 23.25, Time: 0.9487s\n",
      "[16] Improvement: -4.75, Time: 0.944s\n",
      "emission probabilities\n",
      "Parameter containing:\n",
      "tensor([[0.3871, 0.6129]])\n",
      "Parameter containing:\n",
      "tensor([[0.9340, 0.0660]])\n",
      "transition probabilities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3846, 0.6144],\n",
       "        [0.4847, 0.5143]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5000\n",
    "context_length = 1000\n",
    "hidden_sequences = generate_hidden_states(batch_size=batch_size, context_length=context_length)\n",
    "emission_sequences = get_emission(hidden_sequences)\n",
    "\n",
    "# fit the HMM\n",
    "d1 = Categorical([[0.3,0.7]])\n",
    "d2 = Categorical([[0.9,0.1]])\n",
    "\n",
    "model = DenseHMM(distributions=[d1, d2], verbose=True)\n",
    "model.fit(emission_sequences)\n",
    "\n",
    "print(\"emission probabilities\")\n",
    "for dist in model.distributions:\n",
    "    print(dist.probs)\n",
    "\n",
    "print(\"transition probabilities\")\n",
    "np.exp(model.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible that we mixed up emission probabilities and transition probabilities.\n",
    "\n",
    "Let's create a new set of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [01:07<00:00, 14.76it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_next_hidden_state(hidden_states: np.ndarray, transition_matrix: np.ndarray):\n",
    "    # hidden_states is a column vector\n",
    "    # the number of rows is B or batch_size\n",
    "    # hidden_states.shape == (B, 1)\n",
    "    B = hidden_states.shape[0]\n",
    "    assert hidden_states.shape == (B, 1)\n",
    "    \n",
    "    # transition_matrix is a square matrix\n",
    "    # transition_matrix.shape == (H,H)\n",
    "    H = transition_matrix.shape[0]\n",
    "    assert transition_matrix.shape == (H,H)\n",
    "    \n",
    "    # each hidden state will get their corresponding probabilities from transition matrix\n",
    "    # probs.shape == (B, H)\n",
    "    probs = transition_matrix[hidden_states.squeeze(), :]\n",
    "    assert probs.shape == (B, H)\n",
    "    \n",
    "    # generate the next state based on the probs\n",
    "    # next_state.shape == (B, 1)\n",
    "    rng = np.random.default_rng()\n",
    "    next_states = []\n",
    "    for row in probs:\n",
    "        next_state = rng.choice(a=np.arange(H), p=row)\n",
    "        next_states.append(next_state)\n",
    "    return np.array(next_states).reshape(B, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hidden_states(transition_matrix:np.ndarray):\n",
    "    all_hidden_states = np.random.randint(low=0,high=2,size=(batch_size,1))\n",
    "    for _ in tqdm(range(context_length-1)):\n",
    "        # get the last column\n",
    "        prev_hidden_states = all_hidden_states[:,-1].reshape(batch_size, 1)\n",
    "        assert prev_hidden_states.shape == (batch_size, 1)\n",
    "        new_hidden_states = generate_next_hidden_state(hidden_states=prev_hidden_states,transition_matrix=transition_matrix)\n",
    "        assert new_hidden_states.shape == (batch_size, 1)\n",
    "        all_hidden_states = np.concatenate((all_hidden_states, new_hidden_states), axis=1)\n",
    "    assert all_hidden_states.shape == (batch_size, context_length)\n",
    "    return all_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_emissions(hidden_states:np.ndarray, emission_matrix: np.ndarray):\n",
    "    # hidden_states.shape == (B, L)\n",
    "    B = hidden_states.shape[0]\n",
    "    L = hidden_states.shape[1]\n",
    "    \n",
    "    # emission_matrix.shape == (H,E)\n",
    "    # E is the number of possible emissions\n",
    "    H = emission_matrix.shape[0]\n",
    "    E = emission_matrix.shape[0]\n",
    "    \n",
    "    # generate the emissions for each sequence\n",
    "    # all_emissions.shape == (B, L)\n",
    "    rng = np.random.default_rng()\n",
    "    all_emissions = []\n",
    "    for row in tqdm(hidden_states):\n",
    "        probs = emission_matrix[row, :]\n",
    "        # probs.shape == (L, E)\n",
    "        assert probs.shape == (L, E)\n",
    "        emissions = []\n",
    "        for p in probs:\n",
    "            emission_state = rng.choice(a=np.arange(E), p=p)\n",
    "            emissions.append(emission_state)\n",
    "        all_emissions.append(emissions)\n",
    "    all_emissions = np.array(all_emissions)\n",
    "    assert all_emissions.shape == (B, L)\n",
    "    return all_emissions.reshape(B,L,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [01:07<00:00, 14.74it/s]\n",
      "100%|██████████| 5000/5000 [01:05<00:00, 76.13it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5000\n",
    "context_length = 1000\n",
    "\n",
    "transition_matrix = np.array([[0.3,0.7],[0.5,0.5]])\n",
    "emission_matrix = np.array([[0.9,0.1],[0.2,0.8]])\n",
    "\n",
    "hidden_states = generate_hidden_states(transition_matrix=transition_matrix)\n",
    "emission_states = generate_emissions(hidden_states=hidden_states, emission_matrix=emission_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(emission_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Improvement: 90096.5, Time: 0.9656s\n",
      "[2] Improvement: 4806.25, Time: 0.9537s\n",
      "[3] Improvement: 3506.25, Time: 0.9854s\n",
      "[4] Improvement: 2522.75, Time: 0.9729s\n",
      "[5] Improvement: 1814.25, Time: 0.9663s\n",
      "[6] Improvement: 1284.75, Time: 0.9675s\n",
      "[7] Improvement: 836.0, Time: 0.9536s\n",
      "[8] Improvement: 530.5, Time: 0.931s\n",
      "[9] Improvement: 369.5, Time: 0.9518s\n",
      "[10] Improvement: 236.0, Time: 0.9373s\n",
      "[11] Improvement: 124.0, Time: 0.9404s\n",
      "[12] Improvement: 75.5, Time: 0.9349s\n",
      "[13] Improvement: 77.0, Time: 0.9397s\n",
      "[14] Improvement: 18.0, Time: 0.9617s\n",
      "[15] Improvement: 20.5, Time: 0.9439s\n",
      "[16] Improvement: -13.5, Time: 0.9713s\n",
      "emission probabilities\n",
      "Parameter containing:\n",
      "tensor([[0.1989, 0.8011]])\n",
      "Parameter containing:\n",
      "tensor([[0.8694, 0.1306]])\n",
      "transition probabilities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4720, 0.5270],\n",
       "        [0.6801, 0.3189]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the HMM\n",
    "d1 = Categorical([[0.3,0.7]])\n",
    "d2 = Categorical([[0.9,0.1]])\n",
    "\n",
    "model = DenseHMM(distributions=[d1, d2], verbose=True)\n",
    "model.fit(emission_states)\n",
    "\n",
    "print(\"emission probabilities\")\n",
    "for dist in model.distributions:\n",
    "    print(dist.probs)\n",
    "\n",
    "print(\"transition probabilities\")\n",
    "np.exp(model.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! The emission and transition matrices are learnt and are similar to the generator.\n",
    "\n",
    "Now, let's see if we can drop the initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Improvement: -12818.0, Time: 1.251s\n",
      "emission probabilities\n",
      "Parameter containing:\n",
      "tensor([[0., 1.]])\n",
      "Parameter containing:\n",
      "tensor([[1., 0.]])\n",
      "transition probabilities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4613, 0.5377],\n",
       "        [0.5560, 0.4430]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the HMM\n",
    "d1 = Categorical(n_categories=2)\n",
    "d2 = Categorical([[0.9,0.1]])\n",
    "\n",
    "model = DenseHMM(distributions=[d1, d2], verbose=True)\n",
    "model.fit(emission_states)\n",
    "\n",
    "print(\"emission probabilities\")\n",
    "for dist in model.distributions:\n",
    "    print(dist.probs)\n",
    "\n",
    "print(\"transition probabilities\")\n",
    "np.exp(model.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Improvement: -12818.0, Time: 1.248s\n",
      "emission probabilities\n",
      "Parameter containing:\n",
      "tensor([[0., 1.]])\n",
      "Parameter containing:\n",
      "tensor([[1., 0.]])\n",
      "transition probabilities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4613, 0.5377],\n",
       "        [0.5560, 0.4430]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the HMM\n",
    "d1 = Categorical()\n",
    "d2 = Categorical([[0.9,0.1]])\n",
    "\n",
    "model = DenseHMM(distributions=[d1, d2], verbose=True)\n",
    "model.fit(emission_states)\n",
    "\n",
    "print(\"emission probabilities\")\n",
    "for dist in model.distributions:\n",
    "    print(dist.probs)\n",
    "\n",
    "print(\"transition probabilities\")\n",
    "np.exp(model.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Parameter X must have a maximum value below 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m d2 \u001b[38;5;241m=\u001b[39m Categorical()\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m DenseHMM(distributions\u001b[38;5;241m=\u001b[39m[d1, d2], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43memission_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memission probabilities\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dist \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mdistributions:\n",
      "File \u001b[0;32m~/.conda/envs/olmo2/lib/python3.11/site-packages/pomegranate/hmm/_base.py:630\u001b[0m, in \u001b[0;36m_BaseHMM.fit\u001b[0;34m(self, X, sample_weight, priors)\u001b[0m\n\u001b[1;32m    627\u001b[0m \tw_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight[j]\n\u001b[1;32m    628\u001b[0m \tp_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m priors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m priors[j]\n\u001b[0;32m--> 630\u001b[0m \tlogp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpriors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp_\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# Calculate and check improvement and optionally print it\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/olmo2/lib/python3.11/site-packages/pomegranate/hmm/dense_hmm.py:608\u001b[0m, in \u001b[0;36mDenseHMM.summarize\u001b[0;34m(self, X, sample_weight, emissions, priors)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummarize\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, emissions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, priors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    566\u001b[0m \u001b[38;5;250m\t\u001b[39m\u001b[38;5;124;03m\"\"\"Extract the sufficient statistics from a batch of data.\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03m\tThis method calculates the sufficient statistics from optionally\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;124;03m\t\tby a component, not gives a target. Default is None.\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03m\t\"\"\"\u001b[39;00m\n\u001b[0;32m--> 608\u001b[0m \tX, emissions, sample_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memissions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memissions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpriors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpriors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    611\u001b[0m \tt, r, starts, ends, logps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_backward(emissions\u001b[38;5;241m=\u001b[39memissions)\n\u001b[1;32m    613\u001b[0m \tX \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/.conda/envs/olmo2/lib/python3.11/site-packages/pomegranate/hmm/_base.py:707\u001b[0m, in \u001b[0;36m_BaseHMM.summarize\u001b[0;34m(self, X, sample_weight, emissions, priors)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Extract the sufficient statistics from a batch of data.\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \n\u001b[1;32m    670\u001b[0m \u001b[38;5;124;03mThis method calculates the sufficient statistics from optionally\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;124;03m\tThe log probability of each example.\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    705\u001b[0m X \u001b[38;5;241m=\u001b[39m _check_parameter(_cast_as_tensor(X), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, \n\u001b[1;32m    706\u001b[0m \tshape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md), check_parameter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_data)\n\u001b[0;32m--> 707\u001b[0m emissions \u001b[38;5;241m=\u001b[39m \u001b[43m_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memissions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpriors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    710\u001b[0m \tsample_weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mexpand(\n\u001b[1;32m    711\u001b[0m \t\temissions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/olmo2/lib/python3.11/site-packages/pomegranate/hmm/_base.py:28\u001b[0m, in \u001b[0;36m_check_inputs\u001b[0;34m(model, X, emissions, priors)\u001b[0m\n\u001b[1;32m     25\u001b[0m emissions \u001b[38;5;241m=\u001b[39m _check_parameter(_cast_as_tensor(emissions), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memissions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     26\u001b[0m \tndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m emissions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m \temissions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_emission_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpriors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpriors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m emissions\n",
      "File \u001b[0;32m~/.conda/envs/olmo2/lib/python3.11/site-packages/pomegranate/hmm/_base.py:298\u001b[0m, in \u001b[0;36m_BaseHMM._emission_matrix\u001b[0;34m(self, X, priors)\u001b[0m\n\u001b[1;32m    294\u001b[0m e \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty((k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk, n), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m    295\u001b[0m \tdevice\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributions):\n\u001b[0;32m--> 298\u001b[0m \tlogp \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_probability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logp, torch\u001b[38;5;241m.\u001b[39mmasked\u001b[38;5;241m.\u001b[39mMaskedTensor):\n\u001b[1;32m    300\u001b[0m \t\tlogp \u001b[38;5;241m=\u001b[39m logp\u001b[38;5;241m.\u001b[39m_masked_data\n",
      "File \u001b[0;32m~/.conda/envs/olmo2/lib/python3.11/site-packages/pomegranate/distributions/categorical.py:181\u001b[0m, in \u001b[0;36mCategorical.log_probability\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_probability\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m\t\u001b[39m\u001b[38;5;124;03m\"\"\"Calculate the log probability of each example.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m\tThis method calculates the log probability of each example given the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m\t\tThe log probability of each example.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m\t\"\"\"\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \tX \u001b[38;5;241m=\u001b[39m \u001b[43m_check_parameter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_cast_as_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mmax_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_keys\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mcheck_parameter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \tlogps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    186\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md):\n",
      "File \u001b[0;32m~/.conda/envs/olmo2/lib/python3.11/site-packages/pomegranate/_utils.py:192\u001b[0m, in \u001b[0;36m_check_parameter\u001b[0;34m(parameter, name, min_value, max_value, value_sum, value_sum_dim, value_set, dtypes, ndim, shape, check_parameter, epsilon)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parameter, vector):\n\u001b[1;32m    191\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m (parameter \u001b[38;5;241m>\u001b[39m max_value)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 192\u001b[0m \t\t\u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m must have a maximum value below\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m \t\t\t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, max_value))\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m parameter \u001b[38;5;241m>\u001b[39m max_value:\n",
      "\u001b[0;31mValueError\u001b[0m: Parameter X must have a maximum value below 0"
     ]
    }
   ],
   "source": [
    "# fit the HMM\n",
    "# they don't allow both to be empty\n",
    "d1 = Categorical()\n",
    "d2 = Categorical()\n",
    "\n",
    "model = DenseHMM(distributions=[d1, d2], verbose=True)\n",
    "model.fit(emission_states)\n",
    "\n",
    "print(\"emission probabilities\")\n",
    "for dist in model.distributions:\n",
    "    print(dist.probs)\n",
    "\n",
    "print(\"transition probabilities\")\n",
    "np.exp(model.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Improvement: -35394.25, Time: 0.8966s\n",
      "emission probabilities\n",
      "Parameter containing:\n",
      "tensor([[0.4917, 0.5083]])\n",
      "Parameter containing:\n",
      "tensor([[0.4917, 0.5083]])\n",
      "transition probabilities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4995, 0.4995],\n",
       "        [0.4995, 0.4995]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the HMM\n",
    "num_emissions = 2\n",
    "d1 = Categorical([[1/num_emissions]*num_emissions])\n",
    "d2 = Categorical([[1/num_emissions]*num_emissions])\n",
    "\n",
    "model = DenseHMM(distributions=[d1, d2], verbose=True)\n",
    "model.fit(emission_states)\n",
    "\n",
    "print(\"emission probabilities\")\n",
    "for dist in model.distributions:\n",
    "    print(dist.probs)\n",
    "\n",
    "print(\"transition probabilities\")\n",
    "np.exp(model.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Improvement: nan, Time: 0.9774s\n",
      "[2] Improvement: nan, Time: 0.9764s\n",
      "[3] Improvement: nan, Time: 1.96s\n",
      "emission probabilities\n",
      "Parameter containing:\n",
      "tensor([[nan, nan]])\n",
      "Parameter containing:\n",
      "tensor([[nan, nan]])\n",
      "transition probabilities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan],\n",
       "        [nan, nan]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the HMM\n",
    "num_emissions = 2\n",
    "d1 = Categorical([[1/num_emissions]*num_emissions])\n",
    "d2 = Categorical()\n",
    "\n",
    "model = DenseHMM(distributions=[d1, d2], verbose=True, max_iter=3)\n",
    "model.fit(emission_states)\n",
    "\n",
    "print(\"emission probabilities\")\n",
    "for dist in model.distributions:\n",
    "    print(dist.probs)\n",
    "\n",
    "print(\"transition probabilities\")\n",
    "np.exp(model.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Improvement: -12818.0, Time: 1.214s\n",
      "emission probabilities\n",
      "Parameter containing:\n",
      "tensor([[1., 0.]])\n",
      "Parameter containing:\n",
      "tensor([[0., 1.]])\n",
      "transition probabilities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4430, 0.5560],\n",
       "        [0.5377, 0.4613]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the HMM\n",
    "num_emissions = 2\n",
    "d1 = Categorical([[1.0/num_emissions]*num_emissions])\n",
    "d2 = Categorical()\n",
    "\n",
    "model = DenseHMM(distributions=[d1, d2], verbose=True)\n",
    "model.fit(emission_states)\n",
    "\n",
    "print(\"emission probabilities\")\n",
    "for dist in model.distributions:\n",
    "    print(dist.probs)\n",
    "\n",
    "print(\"transition probabilities\")\n",
    "np.exp(model.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the two previous code snippets, we need to cast it as double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Improvement: -35394.25, Time: 0.9203s\n",
      "emission probabilities\n",
      "Parameter containing:\n",
      "tensor([[0.4917, 0.5083]])\n",
      "Parameter containing:\n",
      "tensor([[0.4917, 0.5083]])\n",
      "transition probabilities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4995, 0.4995],\n",
       "        [0.4995, 0.4995]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the HMM\n",
    "num_emissions = 2\n",
    "d1 = Categorical([[1.0/num_emissions]*num_emissions])\n",
    "d2 = Categorical([[1.0/num_emissions]*num_emissions])\n",
    "\n",
    "model = DenseHMM(distributions=[d1,d2], verbose=True)\n",
    "model.fit(emission_states)\n",
    "\n",
    "print(\"emission probabilities\")\n",
    "for dist in model.distributions:\n",
    "    print(dist.probs)\n",
    "\n",
    "print(\"transition probabilities\")\n",
    "np.exp(model.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Improvement: nan, Time: 1.029s\n",
      "[2] Improvement: nan, Time: 1.029s\n",
      "[3] Improvement: nan, Time: 2.058s\n",
      "emission probabilities\n",
      "Parameter containing:\n",
      "tensor([[nan, nan]])\n",
      "Parameter containing:\n",
      "tensor([[nan, nan]])\n",
      "transition probabilities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan],\n",
       "        [nan, nan]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the HMM\n",
    "num_emissions = 2\n",
    "d = [Categorical([[1.0/num_emissions]*num_emissions])] * 2\n",
    "\n",
    "model = DenseHMM(distributions=d, verbose=True, max_iter=3)\n",
    "model.fit(emission_states)\n",
    "\n",
    "print(\"emission probabilities\")\n",
    "for dist in model.distributions:\n",
    "    print(dist.probs)\n",
    "\n",
    "print(\"transition probabilities\")\n",
    "np.exp(model.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Improvement: -35394.25, Time: 0.8969s\n",
      "emission probabilities\n",
      "Parameter containing:\n",
      "tensor([[0.4917, 0.5083]])\n",
      "Parameter containing:\n",
      "tensor([[0.4917, 0.5083]])\n",
      "transition probabilities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4995, 0.4995],\n",
       "        [0.4995, 0.4995]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the HMM\n",
    "num_emissions = 2\n",
    "d = [Categorical([[1.0/num_emissions]*num_emissions]) for _ in range(2)]\n",
    "\n",
    "model = DenseHMM(distributions=d, verbose=True, max_iter=3)\n",
    "model.fit(emission_states)\n",
    "\n",
    "print(\"emission probabilities\")\n",
    "for dist in model.distributions:\n",
    "    print(dist.probs)\n",
    "\n",
    "print(\"transition probabilities\")\n",
    "np.exp(model.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noting the past two snippets, it seems like `[a]*2` will create nan presumably by using the same `Categorical()` across distributions, so we have to explicit call a new one for each distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
