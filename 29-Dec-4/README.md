Goal: train a HMM to speak TinyStories

This is a fresh new copy, taking the advantage of switching the storage to `/n/netscratch`.

The working folder is now `/n/netscratch/sham_lab/Everyone/jchooi/in-context-language-learning`.

This document follows the plan below.
1. Download the TinyStories dataset.
2. Process it so that it only has lower case alphabets, space, commas and periods.
3. Create a dataset using the sentences that only uses the most frequent 500 words.
4. Tokenize and train-test split it.
5. Fit a range of HMMs on the dataset. Record their training and testing loss over time. Manually inspect the output after training.

## Downloading the TinyStories dataset

We looked at the TinyStories huggingface [website](https://huggingface.co/datasets/roneneldan/TinyStories). It mentions a file `TinyStoriesV2-GPT4-train.txt` that is generated by GPT-4. We downloaded it from this [link](https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt?download=true) which has size 2.23 GB.

The file is downloaded to `/n/netscratch/sham_lab/Everyone/jchooi/in-context-language-learning/data/TinyStoriesV2-GPT4-train.txt` by calling `wget`.

You can preview the file with `head -n 50 TinyStoriesV2-GPT4-train.txt`.

## Processing the TinyStories dataset

The TinyStories data is a text file where stories are separated by `<|endoftext|>`. We will remove this since we only care about sentence structure. We process it at `process_tinystories.ipynb`.

We removed all lines that contain characters that are not alphabets or periods or commas or spaces. That includes quotations marks, exclamation marks and numbers.

The resulting file has one sentence on each line. Each sentence ends with the period. Each sentence contains only lowercase alphabets, periods or commas or spaces. Periods and commas are separated from other characters with a space.

Note that the story structure is disrupted because we removed entire lines that include non-accepted characters.

## Subsetting the TinyStories dataset

We rank each word in the processed dataset by frequency. We then plot how much sentences we have if we only allow ourselves to a certain number of words. We do this at `frequent_words.ipynb`.

![](img/count-sentence-vs-words-used.png)

We created datasets that use the top 100, 200, 300, 400, 500 words respectively.

Number of sentences
['1,069,319', '2,836,385', '4,338,458', '5,648,819', '6,783,857']

Unique sentences
['87,200', '517,265', '1,118,802', '1,758,064', '2,384,789']

Duplication rates
[94.74, 86.51, 79.88, 75.0, 71.21]

CHECK WITH ERAN: are the high duplication rates okay?
ANS: Use unique sentences

## Tokenizing the datasets

We tokenize each word in the datasets into its own integer, which is the rank (breaking frequency ties alphabetically). We also create the train-test split for each dataset. This is done in `tokenize_tinystories.ipynb`.

In the train-test split process, we randomly pulled out sentences into either train or test, so the story structure is further disrupted.

Number of training tokens (unique sentences)
['689,997', '4,360,726', '9,625,369', '15,352,822', '20,938,503']

Number of test tokens (unique sentences)
['171,600', '1,089,234', '2,404,845', '3,837,880', '5,238,512']


Number of training tokens
['7,451,413', '20,608,430', '31,785,933', '42,779,104', '51,522,412']

Number of test tokens
['2,128,850', '4,694,522', '7,513,301', '9,463,947', '11,918,012']

## Fitting the HMM

Before traininig all HMM models, we will experiment with the smaller 100-word model. 100-word means that the model has an emission dimension of 100. We will experiment with hidden state dimensions of 100, 200, 400, 800, 1600. We will experiment with context windows 100, 200, 400, 800, 1600. The results will then be tabulated.

This is done in `train_hmm_explore_100.ipynb`.

CHECK WITH ERAN: is there value to track loss over time?
ANS: try with smaller ones, if fast then no need track. Can also integrate with wandb.