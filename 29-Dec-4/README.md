Goal: train a HMM to speak TinyStories

This is a fresh new copy, taking the advantage of switching the storage to `/n/netscratch`.

The working folder is now `/n/netscratch/sham_lab/Everyone/jchooi/in-context-language-learning`.

This document follows the plan below.
1. Download the TinyStories dataset.
2. Process it so that it only has lower case alphabets, space, commas and periods.
3. Create a dataset using the sentences that only uses the most frequent 500 words.
4. Tokenize and train-test split it.
5. Fit a range of HMMs on the dataset. Record their training and testing loss over time. Manually inspect the output after training.

## Downloading the TinyStories dataset

We looked at the TinyStories huggingface [website](https://huggingface.co/datasets/roneneldan/TinyStories). It mentions a file `TinyStoriesV2-GPT4-train.txt` that is generated by GPT-4. We downloaded it from this [link](https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt?download=true) which has size 2.23 GB.

The file is downloaded to `/n/netscratch/sham_lab/Everyone/jchooi/in-context-language-learning/data/TinyStoriesV2-GPT4-train.txt` by calling `wget`.

You can preview the file with `head -n 50 TinyStoriesV2-GPT4-train.txt`.

## Processing the TinyStories dataset

The TinyStories data is a text file where stories are separated by `<|endoftext|>`. We will remove this since we only care about sentence structure. We process it at `process_tinystories.ipynb`.

We removed all lines that contain characters that are not alphabets or periods or commas or spaces. That includes quotations marks, exclamation marks and numbers.

The resulting file has one sentence on each line. Each sentence ends with the period. Each sentence contains only lowercase alphabets, periods or commas or spaces. Periods and commas are separated from other characters with a space.

## Subsetting the TinyStories dataset

We rank each word in the processed dataset by frequency. We then plot how much sentences we have if we only allow ourselves to a certain number of words. For simplicity