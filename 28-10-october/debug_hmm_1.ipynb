{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Situation: the HMMs are outputting \"also\" instead of \" \" every other word\n",
    "\n",
    "This happens during the HMM fit.\n",
    "\n",
    "Question: Is this a problem of encoding?\n",
    "\n",
    "This notebook aims to answer that question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer_lib import *\n",
    "\n",
    "tokenizer_filename = \"tokenizers/simple-600.pkl\"\n",
    "tokenizer = custom_tokenizer(tokenizer_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "also_id = tokenizer.tokenize_word(\"also\")\n",
    "also_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "414"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space_id = tokenizer.tokenize_word(\" \")\n",
    "space_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to `train_hmm.py`, we used the following data to fit the HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[254, 414, 539, ..., 248, 248, 248],\n",
       "       [406, 414, 397, ..., 248, 248, 248],\n",
       "       [406, 414, 397, ..., 248, 248, 248],\n",
       "       [254, 414, 539, ..., 248, 248, 248],\n",
       "       [254, 414, 539, ..., 248, 248, 248]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filename = \"/n/holyscratch01/sham_lab/summer_2024/datasets/cleaned_tiny-600.npy\"\n",
    "data = np.load(data_filename)\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's detokenize the data with the tokenizer to see if they agree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([254, 414, 539, 414, 520, 414, 304, 362, 414, 561, 414, 565, 414,\n",
       "       520, 414,  67, 414, 440, 309, 414, 434, 414, 440, 414, 177, 414,\n",
       "         0, 414, 234, 414,   4, 414, 317, 414, 429, 414,  84, 309, 414,\n",
       "       515, 414, 140, 414, 212, 414, 203, 414, 143, 414, 547, 414, 317,\n",
       "       414, 514, 309, 536, 406, 414, 397, 362, 414, 434, 414, 440, 414,\n",
       "       119, 414, 520, 414,  37, 362, 414, 307, 414, 164, 309, 414, 434,\n",
       "       414, 164, 414, 204, 414,   4, 414, 317, 414, 429, 414,  84, 309,\n",
       "       414,  90, 414, 129, 362, 414, 130, 393, 414,  43, 414,  75, 362,\n",
       "       414, 156, 414, 440, 309, 414, 226, 414, 209, 414, 277, 414,  18,\n",
       "       414,   4, 414, 317, 414, 429, 414, 256, 414, 566, 309, 130, 414,\n",
       "       434, 414, 440, 414, 565, 414, 199, 414, 332, 414, 491, 414, 129,\n",
       "       362, 414, 130, 430, 414,  18, 362, 414,  37, 414, 164, 368, 130,\n",
       "       536, 434, 414, 164, 414, 170, 414, 434, 414, 440, 414, 317, 414,\n",
       "       520, 414,  57, 414, 401, 414, 284, 414, 438, 414, 566, 309, 414,\n",
       "       434, 414, 440, 414, 227, 414, 491, 414, 227, 414, 330, 414, 515,\n",
       "       414, 565, 414,   0, 414,  67, 414, 105, 309, 414, 434, 414, 440,\n",
       "       414, 491, 414, 434, 414, 164, 414, 263, 414, 145, 414, 490, 309,\n",
       "       414,  32, 414, 509, 414,  47, 414, 196, 414, 397, 414, 491, 414,\n",
       "       389, 414, 560, 414, 438, 414, 101, 309, 248, 536, 248, 248, 248,\n",
       "       248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248,\n",
       "       248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248,\n",
       "       248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248,\n",
       "       248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248,\n",
       "       248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248,\n",
       "       248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248,\n",
       "       248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248,\n",
       "       248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248,\n",
       "       248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248,\n",
       "       248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248,\n",
       "       248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248,\n",
       "       248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248,\n",
       "       248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248,\n",
       "       248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248,\n",
       "       248, 248, 248, 248, 248, 248, 248, 248, 248])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_sentence_from_data = data[0]\n",
    "first_sentence_from_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'once upon a time, there was a hungry butterfly. the butterfly did not know where to find food. it flew around looking for something to eat.\\none day, the butterfly met a big, friendly bear. the bear knew where to find food. he said, \"come with me, little butterfly. i will show you where to find yummy flowers.\" the butterfly was very happy and said, \"thank you, big bear!\"\\nthe bear took the butterfly to a beautiful garden full of flowers. the butterfly ate and ate until it was not hungry anymore. the butterfly and the bear became good friends. they played together every day and had lots of fun.<|endoftext|>\\n<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_sentence_detokenized = tokenizer.detokenize_sentence(first_sentence_from_data)\n",
    "first_sentence_detokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first sentence is successfully decoded.\n",
    "\n",
    "Was the question at the beginning answered: Yes\n",
    "Result: The problem is not with the encoding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
