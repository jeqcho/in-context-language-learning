First we train the tokenizer using `train_tokenizer.py`.

Then we tokenize TinyStories using `tokenize_tinystories.py`.

Then we fit the HMMs using `train_hmm.py`.

We have tried to fit the HMMs, but it took too much time (>8 hours for a 1024-tokenizer with max length 1024). See job 46749915.

We are now thinking of scaling down to use 500-tokenizer with max length 500 and the first 1000 rows. This is job 46800667.

Thankfully, job 46798305 ran successfully, which fitted a 100-hmm using 1000-tokenizer of the entire test data with 1024 max length.

First transformer trained successfully, saved at `/n/holyscratch01/sham_lab/summer_2024/checkpoints/1k-vocab-1024-maxlength-100-components-46816664/step1000-unsharded`

Job 46800667 trained a 100-hmm using 500-tokenizer and 500-maxlength.

Weirdly, `hmm-100-test-tokenizer-500-maxlength-500-rows-5000.pkl` gives a emission size of (100,69) but (100,500) expected.

9/17
Realized a bug in the 500-tokenizer: we didn't include spaces. Added the space back in and rerun `tokenize_tinystories.py` and `train_hmm.py`.

We tried `hmm-100-test-tokenizer-1k-maxlength-1024.pkl`. Pretty encouraging results using sampling

> One day, a boy named Tim went to the park to play. He
> Output
> 
> f picked re the,ice the gra theh

Number of tokens per story for 1k-tokenizer
Q3 (75th Percentile): 276.0
90th Percentile: 363.0
99th Percentile: 747.0

Number of tokens per story for 500-tokenizer
Q1 (25th Percentile): 623.0
Q3 (75th Percentile): 852.0
90th Percentile: 1079.0
99th Percentile: 2176.800000000003
Max: 4138

Now training 1k and 500 tokenizer, both using 500 maxlength

### 500-tokenizer, 50 components
hmm-50-test-tokenizer-500-maxlength-500-rows-5000.pkl

```
Input

Once upon a time there was a little boy named Ben. Ben loved to explore the world around him.
Output

 the was n was n was n was n was n was n
---------------
Input

One day, a boy named Tim went to the park to play. He
Output

s n was n was n was n was n was n was n 
---------------
```


### 500-tokenizer, 100 components
hmm-100-test-tokenizer-500-maxlength-500-rows-5000.pkl

```
Input

Once upon a time there was a little boy named Ben. Ben loved to explore the world around him.
Output

 the bar, taid taid taid taid taid taid 
---------------
Input

One day, a boy named Tim went to the park to play. He
Output

y taid taid taid taid taid taid taid tai
---------------
```

### 1k tokenizer, 50 components

```
Input

Once upon a time there was a little boy named Ben. Ben loved to explore the world around him.
Output

 the mom. They was a. They was a
---------------
Input

One day, a boy named Tim went to the park to play. He
Output

 play a. They was a. They was a
---------------
```

### 1k tokenizer, 100 components

```
Input

Once upon a time there was a little boy named Ben. Ben loved to explore the world around him.
Output

<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>
---------------
Input

One day, a boy named Tim went to the park to play. He
Output

very!" JOne bro day and funh.
---------------
```

### 1k tokenizer, 250 components

```
Input

Once upon a time there was a little boy named Ben. Ben loved to explore the world around him.
Output

 rirbs. He was to, in the pc friends. They day, the pring, the pring, the pring, the pring, the p
---------------
Input

One day, a boy named Tim went to the park to play. He
Output

pped it. He was to, in the pc friends. They day, the pring, the pring, the pring, the pring, the p
---------------
```



### sampled, 1k tokenizer, 250 components

```
---------------
Input

Once upon a time there was a
Output

't girl to st other. she could to,,ppst. They started toatass inp with big mom it, the good, if not givece. They dayny dad,
---------------
```

### sampled, 1k tokenizer, 100 components

```
---------------
Input

Once upon a time there was a
Output

 no sawy in thech "He eetppedat." and did, mom.
One rei, butitty wantedouroughtum smiled mom was so "b'mig around the
---------------
```

### sampled, 1k tokenizer, 50 components

```
---------------
Input

Once upon a time there was a
Output

 cov Tim was with sa will. She n much andr went always the spon it saw a happy He walked time.! she<|endoftext|>
<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>
---------------
```

Now training with 828-tokenizer on 1000 rows for many states, and 5000 rows for less states.


### sampled, 500 tokenizer, 50 components, 1000 rows

```
Input

Once upon a time there was a little boy named Ben. Ben loved to explore the world around him.
Output

yd Soy cake. sn an. Th fokkmA!  e  fi'y 
---------------
Input

One day, a boy named Tim went to the park to play. He
Output

p siy, a Lhgoet hots. theoey. Htie did m
---------------
```

### sampled, 500 tokenizer, 50 components, 5000 rows

```
Input

Once upon a time there was a little boy named Ben. Ben loved to explore the world around him.
Output

 the barde
---------------
Input

One day, a boy named Tim went to the park to play. He
Output

teaaheaial
---------------
```


## 26 September
I am trying to see how well can HMM model language, or complete TinyStories.
I have identified two papers that will be central to my research
1. [Scaling Hidden Markov Language Models](https://arxiv.org/pdf/2011.04640)
2. [On Limitation of Transformer for Learning HMMs](https://arxiv.org/pdf/2406.04089)

It could be possible that a transformer trained on HMMs will perform better than HMMs in test-time.

## 27 September
Meeting with Eran suggests that we can use a word-level tokenizer and subset the tinystories to those with those 1k words.